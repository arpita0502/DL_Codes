{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0_5fNcW-GM0",
        "outputId": "358e0d93-728d-401a-8860-4d30aad87f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/afnan47/cuda.git\n",
            "  Cloning https://github.com/afnan47/cuda.git to /tmp/pip-req-build-tlf603bu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/afnan47/cuda.git /tmp/pip-req-build-tlf603bu\n",
            "  Resolved https://github.com/afnan47/cuda.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ],
      "source": [
        "# Set up CUDA\n",
        "#First Change runtime to GPU and run this cell\n",
        "!pip install git+https://github.com/afnan47/cuda.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bubble.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__device__ void device_swap(int& a, int& b) {\n",
        "    int temp = a;\n",
        "    a = b;\n",
        "    b = temp;\n",
        "}\n",
        "\n",
        "__global__ void kernel_bubble_sort_odd_even(int* arr, int size) {\n",
        "    bool isSorted = false;\n",
        "    while (!isSorted) {\n",
        "        isSorted = true;\n",
        "        int tid = blockIdx.x * blockDim.x + threadIdx.x; // calculating global thread id.\n",
        "        if (tid % 2 == 0 && tid < size - 1) {\n",
        "            if (arr[tid] > arr[tid + 1]) {\n",
        "                device_swap(arr[tid], arr[tid + 1]);\n",
        "                isSorted = false;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads(); // Synchronize threads within block\n",
        "        if (tid % 2 != 0 && tid < size - 1) {\n",
        "            if (arr[tid] > arr[tid + 1]) {\n",
        "                device_swap(arr[tid], arr[tid + 1]);\n",
        "                isSorted = false;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads(); // Synchronize threads within block\n",
        "    }\n",
        "}\n",
        "\n",
        "void bubble_sort_odd_even(vector<int>& arr) {\n",
        "    int size = arr.size();\n",
        "    int* d_arr;\n",
        "    cudaMalloc(&d_arr, size * sizeof(int));\n",
        "    cudaMemcpy(d_arr, arr.data(), size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Calculate grid and block dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (size + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Perform bubble sort on GPU\n",
        "    kernel_bubble_sort_odd_even<<<gridSize, blockSize>>>(d_arr, size);\n",
        "\n",
        "    // Copy sorted array back to host\n",
        "    cudaMemcpy(arr.data(), d_arr, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cout << \"sorted array\" << endl;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        cout << arr[i] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    cudaFree(d_arr);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    vector<int> arr = {5, 4, 3, 2, 1, 0, 6, 9, 7};\n",
        "    double start, end;\n",
        "    // Measure performance of parallel bubble sort using odd-even transposition\n",
        "    start = chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "    bubble_sort_odd_even(arr);\n",
        "    end = chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "    cout << \"Parallel bubble sort using odd-even transposition time: \" << end - start << \" milliseconds\" << endl;\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZhe_6Y1-gyv",
        "outputId": "ae940add-f15d-4ce0-bf05-4bc6567d17d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bubble.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc bubble.cu -o bubble"
      ],
      "metadata": {
        "id": "mU4Zr9Pm_F_7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bubble"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdLmM3q_hHd",
        "outputId": "18c3b442-8f0e-460f-d61a-bce2d95e97d5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sorted array\n",
            "0 1 2 3 4 5 6 7 9 \n",
            "Parallel bubble sort using odd-even transposition time: 394 milliseconds\n"
          ]
        }
      ]
    }
  ]
}